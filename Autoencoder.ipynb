{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.linalg import eigvals\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate quantum states\n",
    "Generate *N_states* separable and entangled states and eventually save them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the functions to generate the states and check that they are entangled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hermitian_product_states(size, n_matrices):\n",
    "    \"\"\"Generates a list of random Hermitian product states\n",
    "    of the given dimension\n",
    "    Product states are hermitian matrices with trace 1.\n",
    "    Input:\n",
    "        size: size of the matrices\n",
    "        n_matrices: number of matrices to generate\n",
    "    Output:\n",
    "        product_states: 3D numpy array of product states\n",
    "    \"\"\"\n",
    "\n",
    "    product_states = []\n",
    "    for _ in range(n_matrices):\n",
    "        real_part = np.random.rand(size, size)\n",
    "        imag_part = np.random.rand(size, size)\n",
    "        product_state = real_part + 1j * imag_part\n",
    "        product_state = np.matmul(product_state, product_state.conj().T)\n",
    "        product_state /= np.trace(product_state)\n",
    "        product_states.append(product_state)\n",
    "\n",
    "    return np.array(product_states)\n",
    "\n",
    "\n",
    "def generate_coefficients(n):\n",
    "    \"\"\"\n",
    "    Generates a list of n random coefficients that sum to 1\n",
    "    Input:\n",
    "        n: number of coefficients to generate\n",
    "    Output:\n",
    "        coefficients: numpy array of n coefficients\n",
    "    \"\"\"\n",
    "    rand_numbers = np.random.rand(n)\n",
    "    rand_numbers /= sum(rand_numbers)\n",
    "\n",
    "    return rand_numbers\n",
    "\n",
    "\n",
    "def generate_separable_states(n_matrices, n_states):\n",
    "    \"\"\"Generates a list of random separable states of the given dimension\n",
    "    Input:\n",
    "        dimensions: size of the matrices\n",
    "        n_matrices: number of matrices used to generate the states\n",
    "        n_states: number of separable states to generate\n",
    "    Output:\n",
    "        separable_states: 3D numpy array of separable states,\n",
    "        of size n_states x dimensions^n_matrices\n",
    "    \"\"\"\n",
    "\n",
    "    states = []\n",
    "\n",
    "    for _ in range(n_states):\n",
    "        rhoA = generate_hermitian_product_states(2, n_matrices)\n",
    "        rhoB = generate_hermitian_product_states(2, n_matrices)\n",
    "        coeffs = generate_coefficients(n_matrices)\n",
    "\n",
    "        sep_state = 0\n",
    "\n",
    "        for i in range(n_matrices):\n",
    "            sep_state += coeffs[i] * np.kron(rhoA[i], rhoB[i])\n",
    "\n",
    "        states.append(sep_state)\n",
    "\n",
    "    return th.tensor(states, dtype=th.complex64)\n",
    "\n",
    "\n",
    "# Entanglement check with Peres-Horodecki criterion\n",
    "def is_entangled(rho):\n",
    "    # Check if the density matrix is 4x4\n",
    "    if rho.shape != (4, 4):\n",
    "        raise ValueError(\"The input matrix should be a 4x4 density matrix.\")\n",
    "\n",
    "    # Calculate the partial transpose of the density matrix\n",
    "    pauli_mat_B = np.array([[1, 0], [0, -1]])\n",
    "    identity_mat = np.eye(2)\n",
    "    \n",
    "    transpose_op = np.kron(identity_mat, pauli_mat_B)\n",
    "    rho_T_B = transpose_op @ np.transpose(rho) @ transpose_op\n",
    "    eigenvalues = eigvals(rho_T_B)\n",
    "\n",
    "    return any(eig < 0 for eig in eigenvalues)\n",
    "\n",
    "\n",
    "def generate_entangled_states(n_states):\n",
    "\n",
    "    states = []\n",
    "    i = 0\n",
    "    while i < n_states:\n",
    "        rand_state = np.random.rand(4, 4)\n",
    "\n",
    "        if is_entangled(rand_state):\n",
    "            states.append(rand_state)\n",
    "            i += 1\n",
    "        else:\n",
    "            continue\n",
    "    return th.tensor(states, dtype=th.complex64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_states_train = 10000\n",
    "sep_data = generate_separable_states(10, N_states_train)\n",
    "ent_data = generate_entangled_states(N_states_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the elemnts of the datasets are truly all separable and entangled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Any entangled data in the separable dataset?: \", any(is_entangled(sep_data[i].numpy()) for i in range(N_states_train)))\n",
    "print(\"Are all the data in the entangled dataset entangled?: \", all(is_entangled(ent_data[i].numpy()) for i in range(N_states_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the dataset into a *N_states* $\\times\\, (4 * 4 * 2)$ matrix and create a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reshape(dataset):\n",
    "    \"\"\"Reshapes the data to be used in the neural network\n",
    "    Input:\n",
    "        dataset: dataset of states to reshape\n",
    "    Output:\n",
    "        reshaped_dataset: dataset of states reshaped to be used in the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_reshaped = th.empty(dataset.shape[0], 2 * dataset.shape[1] * dataset.shape[2])\n",
    "    \n",
    "    for i in range(dataset.shape[0]):\n",
    "        dataset_reshaped[i] = th.cat((dataset[i].real.flatten(), dataset[i].imag.flatten()), dim = 0)\n",
    "    \n",
    "    return dataset_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_sep = data_reshape(sep_data)\n",
    "full_data_ent = data_reshape(ent_data)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "sep_train_loader = DataLoader(full_data_sep, batch_size=BATCH_SIZE, shuffle=True)\n",
    "ent_train_loader = DataLoader(full_data_ent, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_states_test = 3000\n",
    "\n",
    "sep_data = generate_separable_states(10, N_states_test)\n",
    "ent_data = generate_entangled_states(N_states_test)\n",
    "\n",
    "print(\"Any entangled data in the separable dataset?: \", any(is_entangled(sep_data[i].numpy()) for i in range(N_states_test)))\n",
    "print(\"Are all the data in the entangled dataset entangled?: \", all(is_entangled(ent_data[i].numpy()) for i in range(N_states_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_sep = data_reshape(sep_data)\n",
    "full_data_ent = data_reshape(ent_data)\n",
    "\n",
    "sep_test_loader = DataLoader(full_data_sep, batch_size=BATCH_SIZE, shuffle=True)\n",
    "ent_test_loader = DataLoader(full_data_ent, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "Perform PCA to analyze the dataset, with the following steps:\n",
    "1. Merge the two datasets into a single one of shape $(2 *$*N_states*$) \\times 32$\n",
    "2. Assign to each row a label: 0 if it represents a separable state, 1 if it represents and entangled one\n",
    "3. Perform PCA and plot the first two principal components and the explained variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two datasets\n",
    "full_dataset = np.concatenate((full_data_sep, full_data_ent), axis=0)\n",
    "\n",
    "# Create the labels\n",
    "label_sep = th.zeros(full_data_sep.shape[0])\n",
    "label_ent = th.ones(full_data_sep.shape[0])\n",
    "labels = th.cat([label_sep, label_ent], dim=0).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "\n",
    "pca_model = PCA()\n",
    "\n",
    "pca = pca_model.fit(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the principal components\n",
    "pca_components = pca.transform(full_dataset)\n",
    "\n",
    "# Plot the first two principal components\n",
    "scatter = plt.scatter(pca_components[:, 0], pca_components[:, 1], c=labels)\n",
    "unique_labels = np.unique(labels)\n",
    "unique_colors = [scatter.cmap(scatter.norm(label)) for label in unique_labels]\n",
    "label_names = ['Separable', 'Entangled']\n",
    "label_name_dict = dict(zip(unique_labels, label_names))\n",
    "patches = [mpatches.Patch(color=unique_colors[i], label=label_name_dict[label]) for i, label in enumerate(unique_labels)]\n",
    "plt.legend(handles=patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the explained variance ratio\n",
    "exp_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "fig = plt.plot(exp_variance, linestyle = '-', marker = '.')\n",
    "plt.xticks(np.arange(1, len(exp_variance) + 1, 2))\n",
    "plt.xlabel(\"Principal components\")\n",
    "plt.ylabel(\"Explained variance ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many principal components have an explained variance ratio greater than a specific threshold. This will be the dimension of the latent space of the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "\n",
    "latent_space_dim = 0\n",
    "for var in exp_variance:\n",
    "    if var > threshold:\n",
    "        latent_space_dim += 1\n",
    "        \n",
    "latent_space_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_fc(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(VAE_fc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Define the encoder layers\n",
    "        self.enc1 = nn.Linear(input_size, hidden_size[0])\n",
    "        self.enc2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.enc_mu = nn.Linear(hidden_size[1], hidden_size[2])\n",
    "        self.enc_logvar = nn.Linear(hidden_size[1], hidden_size[2])\n",
    "        \n",
    "        # Define the decoder layers\n",
    "        self.dec1 = nn.Linear(hidden_size[2], hidden_size[1])\n",
    "        self.dec2 = nn.Linear(hidden_size[1], hidden_size[0])\n",
    "        self.dec3 = nn.Linear(hidden_size[0], input_size)\n",
    "        \n",
    "        # Define the activation function\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    # Define the encoder\n",
    "    def encoder(self, x):\n",
    "        x = self.relu(self.enc1(x))\n",
    "        x = self.relu(self.enc2(x))\n",
    "        mu = self.enc_mu(x)\n",
    "        logvar = self.enc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    # Define the normal distribution\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = th.exp(0.5*logvar)\n",
    "        eps = th.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    # Define the decoder\n",
    "    def decoder(self, z):\n",
    "        z = self.relu(self.dec1(z))\n",
    "        z = self.relu(self.dec2(z))\n",
    "        z = self.relu(self.dec3(z))\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        decoded = self.decoder(z)\n",
    "        # Return the output of the decoder, the latent space vector, and the mean and logvar of the normal distribution\n",
    "        return decoded, z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def loss_function(output, x, mu, logvar):\n",
    "    recon_loss = F.mse_loss(output, x, reduction='sum')\n",
    "    kl_loss = -0.5 * th.sum(1 + logvar - mu * mu - logvar.exp())\n",
    "    return recon_loss + 0.5 * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train the model\n",
    "def train_model(model, train_loader, optimizer, criterion, epochs):\n",
    "    \n",
    "    train_loss = []\n",
    "    for epoch in trange(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for _, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output, _, mu, logvar = model(data)\n",
    "            loss = criterion(output, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "        train_loss.append(epoch_loss)\n",
    "        print(f\"Epoch: {epoch}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    return model, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constant parameters\n",
    "HIDDEN_SIZE = [50, 25, latent_space_dim]\n",
    "EPOCHS = 15\n",
    "LR = 0.001\n",
    "INPUT_SIZE = n_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE on the separable dataset\n",
    "model = VAE_fc(input_size = INPUT_SIZE, hidden_size = HIDDEN_SIZE)\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR)\n",
    "\n",
    "sep_trained_model, sep_train_loss = train_model(model, sep_train_loader, optimizer, loss_function, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train entangled VAE\n",
    "model = VAE_fc(input_size = INPUT_SIZE, hidden_size = HIDDEN_SIZE)\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR)\n",
    "\n",
    "ent_trained_model, ent_train_loss = train_model(model, ent_train_loader, optimizer, loss_function, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the losses for each epoch and their difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sep_train_loss, label='Separable')\n",
    "plt.plot(ent_train_loss, label='Entangled')\n",
    "plt.plot(np.abs(np.array(sep_train_loss) - np.array(ent_train_loss)), label='Difference')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the reconstruction loss on the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = np.ndarray((2, 2))\n",
    "\n",
    "\n",
    "for i, data in enumerate([ent_test_loader.dataset, sep_test_loader.dataset]):\n",
    "    for j, model in enumerate([ent_trained_model, sep_trained_model]):\n",
    "        test_loss[i, j] = F.mse_loss(model(data)[0], data, reduction='sum').detach().numpy() / len(ent_test_loader.dataset)\n",
    "\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = pd.DataFrame(test_loss, columns=['ent_model', 'sep_model'], index=['ent_set', 'sep_set'])\n",
    "\n",
    "test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the two VAE generate the correct kinds of quantum states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to bring the output in the form of a $4 \\times 4$ complex matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_decoded_data(decoded_data):\n",
    "    reconstructed_data = np.ndarray((len(decoded_data), 4, 4), dtype=complex)\n",
    "\n",
    "    for i in range(len(decoded_data)):\n",
    "        decoded = decoded_data[i, 0:16] + 1j * decoded_data[i, 16:]\n",
    "        reconstructed_data[i] = decoded.reshape((4, 4))\n",
    "        \n",
    "    return reconstructed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the outputs of the two autoencoders for both the test sets and reshape them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_ent_model_ent_data = ent_trained_model(ent_test_loader.dataset)[0].detach().numpy()\n",
    "decoded_ent_model_sep_data = ent_trained_model(sep_test_loader.dataset)[0].detach().numpy()\n",
    "\n",
    "decoded_sep_model_sep_data = sep_trained_model(sep_test_loader.dataset)[0].detach().numpy()\n",
    "decoded_sep_model_ent_data = sep_trained_model(ent_test_loader.dataset)[0].detach().numpy()\n",
    "\n",
    "\n",
    "reconstructed_ent_model_ent_data = reconstruct_decoded_data(decoded_ent_model_ent_data)\n",
    "reconstructed_ent_model_sep_data = reconstruct_decoded_data(decoded_ent_model_sep_data)\n",
    "\n",
    "reconstructed_sep_model_sep_data = reconstruct_decoded_data(decoded_sep_model_sep_data)\n",
    "reconstructed_sep_model_ent_data = reconstruct_decoded_data(decoded_sep_model_ent_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the \\% of entangled and separable states in each output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(decoded):\n",
    "    decoded_sep = 0\n",
    "    decoded_ent = 0\n",
    "    for i in range(len(decoded)):\n",
    "        entangled = is_entangled(decoded[i])\n",
    "        \n",
    "        if entangled:\n",
    "            decoded_ent += 1\n",
    "        else:\n",
    "            decoded_sep += 1\n",
    "    const = 100 / (decoded_sep + decoded_ent)\n",
    "    print(\"Entangled data:\", decoded_ent * const, \"%\")\n",
    "    print(\"Separable data:\", decoded_sep * const, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entangled model, entangled data\")\n",
    "check_data(reconstructed_ent_model_ent_data)\n",
    "print()\n",
    "\n",
    "print(\"Entangled model, separable data\")\n",
    "check_data(reconstructed_ent_model_sep_data)\n",
    "print()\n",
    "\n",
    "print(\"Separable model, separable data\")\n",
    "check_data(reconstructed_sep_model_sep_data)\n",
    "print()\n",
    "\n",
    "print(\"Separable model, entangled data\")\n",
    "check_data(reconstructed_sep_model_ent_data)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinguish between separable and entangled states using the VAE\n",
    "1. Generate some labeled states that are entangled or separable at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_states_test = 100\n",
    "\n",
    "true_labels = np.ndarray((N_states_test, 1))\n",
    "states = th.empty((N_states_test, 32), dtype=th.float32)\n",
    "\n",
    "for i in range(N_states_test):\n",
    "    # Choose randomly beween 0 and 1\n",
    "    true_labels[i] = np.random.randint(0, 2)\n",
    "    \n",
    "    if true_labels[i] == 0:\n",
    "        # Generate separable state\n",
    "        state = generate_separable_states(10, 1)\n",
    "    else:\n",
    "        # Generate entangled state\n",
    "        state = generate_entangled_states(1)\n",
    "    \n",
    "    # Flatten the state and concatenate the real and imaginary parts\n",
    "    states[i] = th.cat((state.flatten(start_dim=0).real, state.flatten(start_dim=0).imag), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assing the labelt to the two states based on their loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.ndarray((N_states_test, 1))\n",
    "\n",
    "for i in range(N_states_test):\n",
    "    # Calculate the output of the models\n",
    "    output_sep = sep_trained_model(states[i])[0]\n",
    "    output_ent = ent_trained_model(states[i])[0]\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss_sep = F.mse_loss(output_sep, states[i], reduction='sum').detach().numpy()\n",
    "    loss_ent = F.mse_loss(output_ent, states[i], reduction='sum').detach().numpy()\n",
    "    \n",
    "    # If the loss obtained with the separable model is smaller, then the state is separable\n",
    "    if loss_sep < loss_ent:\n",
    "        predicted_labels[i] = 0 \n",
    "    # Otherwise, the state is entangled\n",
    "    else:\n",
    "        predicted_labels[i] = 1 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy of the predicted labels: {acc*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
